<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Multi-Cali Anything is a dense-feature-driven multi-frame camera calibration method designed for large-scale camera arrays.">
  <meta name="keywords" content="Multi-Cali Anything, Camera Calibration, SfM, COLMAP, Multi-view Stereo">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Multi-Cali Anything: Dense Feature Multi-Frame Structure-from-Motion</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Multi-Cali Anything: Dense Feature Multi-Frame
              Structure-from-Motion for Large-Scale Camera Array Calibration</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="#">Jinjiang You</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="#">Hewei "Stephen" Wang</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="#">Yijie Li</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="#">Mingxiao Huo</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="#">Long Vân Tran Ha</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="#">Mingyuan Ma</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="#">Jinfeng Xu</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="#">Puzhen Wu</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="#">Shubham Garg</a><sup>5</sup>,
              </span>
              <span class="author-block">
                <a href="#">Wei Pu</a><sup>5</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>CMU</span>,
              <span class="author-block"><sup>2</sup>Harvard</span>,
              <span class="author-block"><sup>3</sup>HKU</span>,
              <span class="author-block"><sup>4</sup>Cornell</span>,
              <span class="author-block"><sup>5</sup>Meta Reality Labs</span>,
              <span class="author-block"><sup>*</sup>Equal Contributions</span>
            </div>
            <div class="is-size-4 publication-authors" ,="" style="margin-bottom: 0.5cm;">
              IROS 2025
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2503.00737" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.00737" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/YJJfish/Multi-Cali-Anything"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="./static/images/architecture.png" alt="Multi-Cali Anything Architecture"
          style="width: 100%; max-width: 800px; display: block; margin: 0 auto;">
        <h2 class="content has-text-centered">
          The overall pipeline of our proposed method. Inputs include multi-frame, multi-view
          images, and camera extrinsics, with outputs being camera intrinsics and SfM sparse reconstructions. (a) to
          (d) illustrate several key components.
        </h2>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-reprojection">
            <img src="./static/images/Reprojection.png" alt="Reprojection Errors Comparison" style="width: 100%;">
          </div>
          <div class="item item-mvs">
            <img src="./static/images/MVS.png" alt="Multi-View Stereo Results" style="width: 100%;">
          </div>
          <div class="item item-dust3r">
            <img src="./static/images/DUSt3R.png" alt="DUSt3R Reconstruction Results" style="width: 100%;">
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small" id="abstract">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Calibrating large-scale camera arrays, such as those used in dome-based setups, is time-intensive and
              often relies on dedicated checkerboard captures. While extrinsic parameters are typically fixed due to the
              physical structure, intrinsic parameters can vary across sessions because of lens adjustments or
              environmental factors like temperature. We introduce <b>Multi-Cali Anything</b>, a dense-feature-driven,
              multi-frame calibration method tailored for large-scale camera arrays. Unlike traditional methods, our
              approach refines intrinsics directly from scene data—eliminating the need for additional calibration
              captures. Built as a plug-and-play add-on to existing Structure-from-Motion (<b>SfM</b>) pipelines (e.g.,
              <b>COLMAP</b>,
              <b>Pixel-Perfect SfM</b>), Multi-Cali Anything utilizes sparse reconstruction results and enhances them
              through
              dense feature refinement. Our method incorporates: (1) an extrinsics regularization term to progressively
              align estimated extrinsics with ground-truth values, (2) a dense feature reprojection loss to reduce
              keypoint errors in the feature space, and (3) an intrinsics variance term to ensure consistency across
              multiple frames. Experiments on the <b>Multiface</b> dataset demonstrate that our method achieves
              calibration
              precision comparable to dedicated calibration procedures, while significantly improving intrinsic
              parameter estimates and 3D reconstruction accuracy. Efficient, scalable, and fully compatible with
              existing SfM workflows, Multi-Cali Anything offers a practical solution for challenging calibration
              scenarios where traditional methods are impractical or infeasible.<br><br><br>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <section class="section" id="method">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 has-text-centered">Why Multi-Cali Anything?</h2>

          <div class="columns is-centered">
            <div class="column">
              <div class="content">
                <h3 class="title is-4">Key Advantages</h3>
                <ul>
                  <li><strong>✅ No extra calibration captures needed</strong> – Uses scene data instead of calibration
                    patterns.</li>
                  <li><strong>✅ Seamless integration</strong> – Works as an add-on to existing SfM pipelines (e.g.,
                    COLMAP, Pixel-Perfect SfM).</li>
                  <li><strong>✅ Dense feature refinement</strong> – Reduces keypoint errors for higher calibration
                    accuracy.</li>
                  <li><strong>✅ Multi-frame optimization</strong> – Ensures consistent intrinsics across multiple
                    frames.</li>
                  <li><strong>✅ Efficient processing</strong> - Suitable for large-scale camera arrays with multiple
                    frame captures.</li>
                  <li><strong>✅ High accuracy</strong> - Achieves nearly the same precision as dedicated calibration
                    processes.</li>
                </ul>
              </div>
            </div>
          </div>

          <div class="columns is-centered">
            <div class="column">
              <div class="content">
                <h3 class="title is-4">How It Works</h3>
                <p> First, users run <strong>any SfM pipeline</strong>, such as COLMAP or Pixel-Perfect SfM, to generate
                  initial sparse reconstructions, including camera parameters and sparse 3D models. Then, they apply
                  <strong>Multi-Cali Anything</strong> to refine the intrinsics and generate improved 3D reconstructions
                  using dense feature optimization.
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="results">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 has-text-centered">Results Gallery</h2>

          <!-- Reprojection Errors -->
          <div class="columns is-centered">
            <div class="column">
              <div class="content">
                <h3 class="title is-4">Reprojection Errors</h3>
                <p>
                  Visualization of reprojection errors using intrinsics estimated by different methods:
                  <strong>COLMAP</strong> (1<sup>st</sup> row), <strong>Pixel-Perfect SfM</strong> (2<sup>nd</sup> row),
                  <strong>VGGSfM</strong> (3<sup>rd</sup> row), and <strong>our method</strong> (4<sup>th</sup> row).
                  A few 3D points from the ground-truth mesh are projected onto images using both ground-truth
                  intrinsics
                  (<strong><span style="color: rgb(255, 0, 0) !important;">red crossings</span></strong>) and estimated
                  intrinsics
                  (<strong><span style="color: rgb(1, 169, 0) !important;">green dots</span></strong>).
                  Our method produces the smallest reprojection error, demonstrating superior calibration accuracy.
                </p>
                <img src="./static/images/Reprojection.png" alt="Reprojection Errors Comparison" style="width: 100%;">
              </div>
            </div>
          </div>

          <!-- Multi-View Stereo -->
          <div class="columns is-centered">
            <div class="column">
              <div class="content">
                <h3 class="title is-4">Multi-View Stereo Reconstructions</h3>
                <p>
                  Comparative visualization of multi-view stereo (MVS) reconstruction using the COLMAP MVS pipeline with
                  intrinsics
                  from <strong>COLMAP</strong>, <strong>Pixel-Perfect SfM</strong>, <strong>VGGSfM</strong>, and
                  <strong>our method</strong>. Point-to-ground-truth mesh distances are computed and color-encoded:
                  <strong><span style="color: red !important;">red</span></strong> for
                  negative deviation,
                  <strong><span style="color: blue !important;">blue</span></strong> for
                  positive deviation,
                  and
                  <strong><span style="color: rgb(1, 169, 0) !important;">green</span></strong>
                  for
                  near-zero error.
                  Our method shows more green points and tighter error distributions in histograms,
                  indicating better alignment with the ground-truth and superior intrinsic calibration.
                </p>
                <img src="./static/images/MVS.png" alt="Multi-View Stereo Results" style="width: 100%;">
              </div>
            </div>
          </div>

          <!-- DUSt3R -->
          <div class="columns is-centered">
            <div class="column">
              <div class="content">
                <h3 class="title is-4">DUSt3R Reconstructions</h3>
                <p>
                  Comparative DUSt3R reconstruction results using ground-truth extrinsics.
                  All models are aligned and scaled to the same pose for visualization.
                  Reconstructions with <strong>DUSt3R’s intrinsics</strong> show significant noise,
                  depth inconsistencies, and scale mismatch. In contrast, reconstructions using
                  <strong>our refined intrinsics</strong> yield more accurate geometry and consistent scale,
                  demonstrating improved reconstruction quality for both heads.
                </p>
                <img src="./static/images/DUSt3R.png" alt="DUSt3R Reconstruction Results" style="width: 100%;">
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{you2025multi,
    title={Multi-Cali Anything: Dense Feature Multi-Frame Structure-from-Motion for Large-Scale Camera Array Calibration},
    author={You, Jinjiang and Wang, Hewei and Li, Yijie and Huo, Mingxiao and Ha, Long Van Tran and Ma, Mingyuan and Xu, Jinfeng and Wu, Puzhen and Garg, Shubham and Pu, Wei},
    journal={arXiv preprint arXiv:2503.00737},
    year={2025}
}
      </code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/pdf/2503.00737">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/YJJfish/Multi-Cali-Anything" class="external-link">
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>